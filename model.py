# model.py
# This script fetches research papers from arXiv based on a specified topic, then uses language models to summarize 
# the papers and answer questions about them. It leverages Hugging Face's transformers library for NLP processing.

import requests
from bs4 import BeautifulSoup
from transformers import pipeline, GPTNeoForCausalLM, AutoTokenizer

# Initialize the language models and pipelines for processing text data
model_name = "EleutherAI/gpt-neo-125M"  # Specifies the pre-trained model to use
tokenizer = AutoTokenizer.from_pretrained(model_name)  # Loads the tokenizer for our chosen model
model = GPTNeoForCausalLM.from_pretrained(model_name)  # Loads the model for text generation (GPT-Neo)

# Summarization and question-answering pipelines
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")  # Loads BART for text summarization
qa_pipeline = pipeline("question-answering")  # Loads a general-purpose question-answering pipeline

def fetch_papers(query, max_results=5):
    """
    Fetch research papers from arXiv based on a topic query.

    Args:
    query (str): The research topic to search for.
    max_results (int): Maximum number of papers to retrieve (default is 5).

    Returns:
    list: A list of dictionaries containing key details about each paper, like title, summary, authors, and publication date.
    """
    # Build the search query for arXiv's API
    base_url = "http://export.arxiv.org/api/query?"
    search_query = f"search_query=all:{query}&start=0&max_results={max_results}"
    response = requests.get(base_url + search_query)
    
    # Check if the response was successful
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, features="xml")
        papers = []
        entries = soup.find_all('entry')  # Each entry represents a paper
        
        # Parse relevant details from each paper entry
        for entry in entries:
            paper_info = {
                "title": entry.title.text,
                "summary": entry.summary.text,
                "authors": [author.text for author in entry.find_all('author')],
                "published": entry.published.text
            }
            papers.append(paper_info)
        return papers
    else:
        # Return an empty list if there was an error with the request
        return []

def summarize_paper(paper_summary):
    """
    Generate a brief summary of the paper's content.

    Args:
    paper_summary (str): The abstract or summary of the paper.

    Returns:
    str: A shortened, concise version of the provided summary text.
    """
    # Use the summarizer pipeline to condense the summary
    summary = summarizer(paper_summary, max_length=130, min_length=30, do_sample=False)
    return summary[0]['summary_text']  # Return the text of the summary

def answer_question(question, context):
    """
    Provide an answer to a question based on the context.

    Args:
    question (str): The question we want to answer.
    context (str): The context (e.g., paper summary) from which the answer will be derived.

    Returns:
    str: The answer generated by the model based on the context.
    """
    # Use the question-answering pipeline to find an answer in the context
    result = qa_pipeline(question=question, context=context)
    return result["answer"]  # Return the extracted answer
